
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmt_m8CgUro8"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Label encoding and train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_mapping = {'FAKE': 0, 'REAL': 1}\n",
        "df['label_num'] = df['label'].map(label_mapping)\n",
        "\n",
        "X = df['clean_text'].values\n",
        "y = df['label_num'].values\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "len(X_train_text), len(X_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: TF-IDF vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))  # unigrams + bigrams\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf  = tfidf.transform(X_test_text)\n",
        "\n",
        "X_train_tfidf.shape, X_test_tfidf.shape"
      ],
      "metadata": {
        "id": "ZmaV56I1VpT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Install gensim (only first time in Colab) and tokenize\n",
        "!pip install gensim\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenize clean text into lists of words\n",
        "df['tokens'] = df['clean_text'].apply(lambda x: x.split())\n",
        "\n",
        "sentences = df['tokens'].tolist()\n",
        "sentences[:2]"
      ],
      "metadata": {
        "id": "EAyenJPYVsPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Train Word2Vec\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,   # embedding size\n",
        "    window=5,\n",
        "    min_count=2,       # ignore very rare words\n",
        "    workers=4,\n",
        "    sg=1               # 1 = skip-gram, 0 = CBOW\n",
        ")\n",
        "\n",
        "w2v_model.wv.key_to_index.__len__()  # vocabulary size"
      ],
      "metadata": {
        "id": "LStcav4tVz5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create document vectors by averaging word vectors\n",
        "import numpy as np\n",
        "\n",
        "def document_vector(tokens, model):\n",
        "    # filter tokens that exist in the model\n",
        "    tokens = [w for w in tokens if w in model.wv]\n",
        "    if len(tokens) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(model.wv[tokens], axis=0)\n",
        "\n",
        "doc_vectors = np.array([document_vector(tokens, w2v_model) for tokens in df['tokens']])\n",
        "doc_vectors.shape"
      ],
      "metadata": {
        "id": "pJW_mJBfV0Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Train-test split using Word2Vec features\n",
        "X_w2v = doc_vectors\n",
        "y_w2v = df['label_num'].values\n",
        "\n",
        "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
        "    X_w2v, y_w2v, test_size=0.2, random_state=42, stratify=y_w2v\n",
        ")\n",
        "\n",
        "X_train_w2v.shape, X_test_w2v.shape\n"
      ],
      "metadata": {
        "id": "i4VOPVxSV3c5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
