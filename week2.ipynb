{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_osXWwdjoOT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Basic info & cleanup\n",
        "df.info()\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Drop useless index column if present\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Combine title and text into one field for NLP\n",
        "df['content'] = df['title'].fillna('') + \" \" + df['text'].fillna('')\n",
        "\n",
        "df[['title', 'text', 'content', 'label']].head()"
      ],
      "metadata": {
        "id": "SBCXfHOwjysU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "    # remove urls\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
        "    # keep only letters\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    # remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # tokenize\n",
        "    words = text.split()\n",
        "    # remove stopwords & lemmatize\n",
        "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['clean_text'] = df['content'].apply(clean_text)\n",
        "df[['content', 'clean_text']].head()"
      ],
      "metadata": {
        "id": "o5Ljl44Zj1o1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
